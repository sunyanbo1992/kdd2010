{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'algebra_2005_2006/algebra_2005_2006_train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b562708f662e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtesting_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'algebra_2005_2006/algebra_2005_2006_train.csv'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcsvfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mfilereader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsvfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquotechar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"|\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'algebra_2005_2006/algebra_2005_2006_train.csv'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "import csv\n",
    "i = 0\n",
    "\n",
    "all_data = []\n",
    "training_set = []\n",
    "labels = []\n",
    "testing_set = []\n",
    "\n",
    "with open('algebra_2005_2006/algebra_2005_2006_train.csv') as csvfile:\n",
    "    filereader = csv.reader(csvfile, delimiter='\\t', quotechar=\"|\")\n",
    "    \n",
    "    for row in filereader:\n",
    "        \n",
    "        if i > 0:\n",
    "            train_data = []\n",
    "            if len(row[10]) == 0:\n",
    "                train_data.append(0)\n",
    "            else:\n",
    "                train_data.append(float(row[10]))\n",
    "            train_data.append(float(row[14]))\n",
    "            train_data.append(float(row[15]))\n",
    "            train_data.append(float(row[16]))\n",
    "            train_data = np.asarray(train_data)\n",
    "            label = np.asarray([0, 0, row[13]])\n",
    "            all_data.append(train_data)\n",
    "            labels.append(label)\n",
    "        \n",
    "        i += 1\n",
    "        if i == 100001:\n",
    "            break\n",
    "all_data = np.asarray(all_data)\n",
    "labels = np.asarray(labels)\n",
    "\n",
    "training_set = all_data[:90000]\n",
    "testing_set = all_data[90000:]\n",
    "\n",
    "training_label = labels[:90000]\n",
    "testing_label = labels[90000:]\n",
    "\n",
    "learning_rate = 0.001\n",
    "training_steps = 1000\n",
    "batch_size = 10\n",
    "display_step = 100\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 4 # MNIST data input (img shape: 28*28)\n",
    "timesteps = 1000 # timesteps\n",
    "num_hidden = 512 # hidden layer num of features\n",
    "num_classes = 3 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "#seq_len = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([num_hidden, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}\n",
    "\n",
    "#inputs = tf.placeholder(tf.float32, [None, None, num_features])\n",
    "\n",
    "cells = []\n",
    "for _ in range(3):\n",
    "    cell = tf.nn.rnn_cell.GRUCell(num_hidden)\n",
    "    #cell = tf.contrib.rnn.AttentionCellWrapper(cell, attn_length = 60, state_is_tuple=True)\n",
    "    cells.append(cell)\n",
    "    \n",
    "stack = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "#stack = ExtendedMultiRNNCell(cells, residual_connections = False, residual_combiner=\"mean\")\n",
    "outputs, _ = tf.nn.dynamic_rnn(stack, X, dtype=tf.float32)\n",
    "\n",
    "'''\n",
    "def RNN(x, weights, biases):\n",
    "\n",
    "    x = tf.unstack(x, timesteps, 1)\n",
    "    #x = tf.nn.dropout(x, 0.8)\n",
    "    #x = tf.transpose(x, [1, 0, 2])\n",
    "\n",
    "    # Define a lstm cell with tensorflow\n",
    "    cell = rnn.GRUCell(num_hidden)\n",
    "    #cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=0.8)\n",
    "    #cell = tf.nn.rnn_cell.MultiRNNCell([cell] * 3)\n",
    "    outputs, states = rnn.static_rnn(cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "'''\n",
    "logits = tf.layers.dense(outputs, num_classes)\n",
    "#logits = RNN(X, weights, biases)\n",
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(1, training_steps+1):\n",
    "        \n",
    "        for i in xrange(9):\n",
    "            batch_x = training_set[i*10000:i*10000 + 10000]\n",
    "            batch_y = training_label[i*10000:i*10000 + 10000]\n",
    "            batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
    "            sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "            right = sess.run(prediction, feed_dict={X: batch_x, Y: batch_y})\n",
    "\n",
    "            batch_y = batch_y.reshape(10000, num_classes)\n",
    "            right = right.reshape(10000, num_classes)\n",
    "            count = 0\n",
    "            for j in xrange(len(right)):\n",
    "                if abs(float(right[j][2]) - float(batch_y[j][2])) < 0.1:\n",
    "                    count += 1\n",
    "            acc = count / 10000.0\n",
    "            if step % display_step == 0 or step == 1:\n",
    "                loss = sess.run(loss_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "                print \"Step \" + str(step) + \", Minibatch Loss= \" + \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \"{:.3f}\".format(acc)\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    test_data = testing_set.reshape(-1, timesteps, num_input)\n",
    "    \n",
    "    print(\"Testing Accuracy:\", sess.run(accuracy, feed_dict={X: test_data, Y: testing_label}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
